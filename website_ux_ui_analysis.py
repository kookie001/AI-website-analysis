# -*- coding: utf-8 -*-
"""Website Ux/ui analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eeJ6N1F2OFUn-_shvzQQKpKdrgWGI9tq
"""

from IPython import get_ipython
from IPython.display import display

# Install required packages
!pip install gradio transformers sentencepiece pyppeteer opencv-python-headless matplotlib requests beautifulsoup4 fpdf nest_asyncio scikit-image
!apt-get update
!apt-get install -y chromium-browser
!npm install -g lighthouse

import asyncio
import os
import subprocess
import json
import cv2
import numpy as np
import matplotlib.pyplot as plt
from fpdf import FPDF
from bs4 import BeautifulSoup
import requests
from skimage.metrics import structural_similarity as ssim
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import nest_asyncio
import gradio as gr
import uuid  # Import uuid for generating unique filenames
from PIL import Image  # Import PIL for image handling

!huggingface-cli login
from huggingface_hub import login

# Replace 'YOUR_TOKEN_HERE' with your actual Hugging Face token
login(token="HUGGIN_FACE_TOKEN")

# Load Hugging Face Flan-T5 model
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

# Function to validate URL
def validate_url(url):
    if not url.startswith("http://") and not url.startswith("https://"):
        raise ValueError("Invalid URL. Ensure it starts with http:// or https://")

# Function to capture screenshots
async def capture_screenshot(url, output_file):
    from pyppeteer import launch
    try:
        browser = await launch(args=['--no-sandbox', '--disable-setuid-sandbox'])
        page = await browser.newPage()
        await page.goto(url, {'waitUntil': 'networkidle2'})
        await page.screenshot({'path': output_file, 'fullPage': True})
        await browser.close()
    except Exception as e:
        print(f"Error capturing screenshot: {e}")

# Function to annotate screenshot
def annotate_screenshot(input_file, output_file):
    try:
        image = cv2.imread(input_file)
        if image is None:
            raise ValueError(f"Error: Could not read the image from {input_file}")
        height, width, _ = image.shape
        cv2.rectangle(image, (10, 10), (width - 10, height - 10), (0, 255, 0), 5)
        cv2.putText(image, 'Annotated Screenshot', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        cv2.imwrite(output_file, image)
        print(f"✅ Annotated screenshot saved to {output_file}")
    except Exception as e:
        print(f"Error in annotating screenshot: {e}")

!pip install seaborn pandas

# Function to generate realistic heatmap
def generate_realistic_heatmap(soup, width, height, output_file):
    heatmap = np.zeros((height, width))
    focus_elements = soup.find_all(["button", "img", "a"])
    for element in focus_elements[:10]:
        x, y = np.random.randint(0, width), np.random.randint(0, height)
        heatmap[y:y+50, x:x+50] += 1
    heatmap += np.random.rand(height, width) * 0.1  # Add noise
    plt.imshow(heatmap, cmap='hot', interpolation='nearest')
    plt.colorbar()
    plt.title("website Heatmap")
    plt.savefig(output_file)
    plt.close()

# Function to calculate layout shift using SSIM
def calculate_layout_shift(before_file, after_file):
    try:
        img1 = cv2.imread(before_file, cv2.IMREAD_GRAYSCALE)
        img2 = cv2.imread(after_file, cv2.IMREAD_GRAYSCALE)
        score, _ = ssim(img1, img2, full=True)
        return score
    except Exception as e:
        print(f"Error calculating layout shift: {e}")
        return 0

# Function to extract content and detect missing elements
def extract_content_and_check_elements(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    text = soup.get_text(strip=True)
    missing_elements = [tag for tag in ["button", "nav", "footer"] if not soup.find(tag)]
    return text, soup, missing_elements

# Function to detect low-quality images
def detect_low_quality_images(soup, url):
    low_quality_images = []
    for img in soup.find_all("img"):
        img_url = img.get("src")
        if img_url:
            img_url = img_url if img_url.startswith("http") else url + img_url
            try:
                img_data = requests.get(img_url, stream=True).content
                img_cv = cv2.imdecode(np.frombuffer(img_data, np.uint8), cv2.IMREAD_GRAYSCALE)
                if img_cv is None or cv2.Laplacian(img_cv, cv2.CV_64F).var() < 50:
                    low_quality_images.append(img_url)
            except Exception as e:
                print(f"Error processing image {img_url}: {e}")
    return low_quality_images

# Function to run Lighthouse audit
def run_lighthouse(url, output_file):
    # Use npx to execute lighthouse directly without relying on global installation
    command = ["npx", "lighthouse", url, "--output=json", f"--output-path={output_file}"]
    result = subprocess.run(command, capture_output=True, text=True)

    if result.returncode != 0:
        print(f"Lighthouse error: {result.stderr}")
        # Instead of returning None, return an empty dictionary
        return {}

    # Corrected indentation for 'with' statement
    with open(output_file, 'r') as f:
        # Parse the JSON data
        data = json.load(f)
    return data

# Function to visualize Lighthouse metrics
def visualize_lighthouse_metrics(metrics, output_file):
    labels = list(metrics.keys())
    values = list(metrics.values())

    plt.bar(labels, values, color='skyblue')
    plt.xlabel('Metrics')
    plt.ylabel('Scores')
    plt.title('Lighthouse Metrics')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(output_file)
    plt.close()

import re
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Load the Flan-T5 model
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def generate_ai_insights(website_url, lighthouse_data, heatmap_data, website_metrics):
    # Step 1: Convert Lighthouse & Heatmap Data into Plain Text
    lighthouse_summary = "\n".join([f"- {k}: {v}" for k, v in lighthouse_data.items()])
    heatmap_summary = f"- Key Interactions: {heatmap_data}" if heatmap_data else "No heatmap data available."
    performance_summary = "\n".join([f"- {k}: {v}" for k, v in website_metrics.items()])

    # Step 2: Simple & Direct Prompt for Flan-T5
    prompt = f"""
    You are an expert web analyst. Based on the given performance and user interaction data, analyze the website {website_url}.

    ## Website Data:
    **Lighthouse Report:**
    {lighthouse_summary}

    **Heatmap Analysis:**
    {heatmap_summary}

    **Performance Metrics:**
    {performance_summary}

    ## Your Task:
    1. List 3-5 strengths of the website under **Pros**.
    2. List 3-5 weaknesses under **Cons**.
    3. Provide 3-5 actionable recommendations under **Recommendations**.
    4. Give a **Website Rating (out of 10)**.

    **Example Response Format:**
    **Pros:**
    - Fast load time
    - Mobile-friendly UI

    **Cons:**
    - Poor navigation
    - Low contrast text

    **Recommendations:**
    - Improve navigation bar
    - Increase text contrast

    **Website Rating:** 8/10

    Now generate the analysis.
    """

    # Step 3: Run Flan-T5 Model
    inputs = tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True)
    outputs = model.generate(inputs.input_ids, max_length=512, num_beams=5, early_stopping=True)
    ai_insights_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Step 4: Extract Rating Using Regex
    rating_match = re.search(r"Website Rating:\s*(\d{1,2})", ai_insights_text)
    rating = int(rating_match.group(1)) if rating_match else None

    return ai_insights_text, rating

# Function to generate PDF report
def generate_pdf_report(title, sections, output_file, images):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.set_font("Arial", style="B", size=16)
    pdf.cell(200, 10, txt=title, ln=True, align='C')
    for section_title, content in sections.items():
        pdf.set_font("Arial", style="B", size=14)
        pdf.cell(200, 10, txt=section_title, ln=True, align='L')
        pdf.set_font("Arial", size=12)
        # Check if content is a tuple and convert it to a string
        if isinstance(content, tuple):
            content = str(content)  # or content[0] if you only want the text part
        pdf.multi_cell(0, 10, content.encode('utf-8').decode('latin-1', 'ignore'))
    for image in images:
        pdf.add_page()
        pdf.image(image, x=10, y=10, w=180)
    pdf.output(output_file)

"""main"""

# Main Function
def main():
    website_url = input("Enter the website URL: ").strip()
    validate_url(website_url)  # validate_url is now defined and accessible

    # File paths
    before_file = "screenshot_before.png"
    after_file = "screenshot_after.png"
    annotated_file = "annotated_screenshot.png"
    heatmap_file = "heatmap.png"
    lighthouse_file = "lighthouse_report.json"
    lighthouse_image = "lighthouse_metrics.png"
    pdf_file = "website_analysis_report.pdf"

    # Apply nest_asyncio to integrate with Jupyter's event loop
    nest_asyncio.apply()

    # Use asyncio.run to execute the coroutines within the existing loop
    asyncio.run(asyncio.gather(
        capture_screenshot(website_url, before_file),
        capture_screenshot(website_url, after_file)
    ))

    layout_shift_score = calculate_layout_shift(before_file, after_file)
    annotate_screenshot(after_file, annotated_file)
    text, soup, missing_elements = extract_content_and_check_elements(website_url)
    low_quality_images = detect_low_quality_images(soup, website_url)
    generate_realistic_heatmap(soup, 800, 600, heatmap_file)

    # Run Lighthouse and visualize metrics
    lighthouse_data = run_lighthouse(website_url, lighthouse_file)
    metrics = {"Performance": 0.9, "Accessibility": 0.8, "Best Practices": 0.85, "SEO": 0.95}
    visualize_lighthouse_metrics(metrics, "lighthouse_metrics.png")


    # Call visualize_lighthouse_metrics before generate_pdf_report
    visualize_lighthouse_metrics(metrics, lighthouse_image)

    issues = f"Missing Elements: {missing_elements}, Low-Quality Images: {low_quality_images}"

    # Get website functionality and user expectations
    website_functionality = "Ecommerce platform selling electronics."  # Example
    user_expectations = "Improve loading speed, ensure accessibility for visually impaired users."  # Example

    # Placeholder for extracting PDF content (if applicable)
    pdf_content = ""

    # Call AI Insights with structured data
    ai_insights, website_rating = generate_ai_insights(
        website_url,
        lighthouse_data,  # Pass Lighthouse analysis results
        "User clicks mostly on CTA buttons, but navigation menu is underused.",  # Example heatmap data
        {"Load Time": 3.2}  # Example website metric
    )

    # Store AI insights in the report - this line likely caused the error
    # sections["AI Insights"] = ai_insights # This line was trying to access 'sections'
    # before it was defined. Moved it after the definition of 'sections'.


    sections = {  # Corrected indentation here
        "Website URL": website_url,
        "Lighthouse Metrics": str(metrics),
        "Layout Shift Score": f"{layout_shift_score:.4f}",
        "AI Insights": ai_insights, # 'AI Insights' is now added directly
        "Missing Elements": ", ".join(missing_elements) if missing_elements else "None",
        "Low-Quality Images": ", ".join(low_quality_images) if low_quality_images else "None"
    }

    generate_pdf_report("Website Analysis Report", sections, pdf_file,
                        [before_file, after_file, heatmap_file, annotated_file, lighthouse_image])

    print(f"✅ Analysis Complete! Report saved as {pdf_file}")

# Ensure main function is called when script is run directly
if __name__ == "__main__":
    main()



"""UI flow start here - do not make any changes in above code"""
